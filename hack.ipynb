{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069d393b",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6969e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "import os\n",
    "DATA_DIR = 'data'\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'val')\n",
    "os.makedirs(os.path.join(DATA_DIR, 'val'), exist_ok=True)\n",
    "EVAL_DAYS_TRESHOLD = 14\n",
    "\n",
    "df_test_users = pl.read_parquet(os.path.join(DATA_DIR, 'test_users.pq'))\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, 'clickstream.pq'))\n",
    "\n",
    "#df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "#df_text_features = pl.read_parquet(os.path.join(DATA_DIR, 'clickstream.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f167539",
   "metadata": {},
   "outputs": [],
   "source": [
    "treshhold = df_clickstream['event_date'].max() - timedelta(days=EVAL_DAYS_TRESHOLD)\n",
    "\n",
    "df_train = df_clickstream.filter(df_clickstream['event_date']<= treshhold)\n",
    "df_eval = df_clickstream.filter(df_clickstream['event_date']> treshhold)[['cookie', 'node', 'event']]\n",
    "\n",
    "df_eval = (\n",
    "        df_eval\n",
    "        .join(df_train, on=['cookie', 'node'], how='anti')\n",
    "        .filter(\n",
    "                pl.col('event').is_in(\n",
    "                    df_event.filter(pl.col('is_contact')==1)['event'].unique()\n",
    "                )\n",
    "            )\n",
    "        .filter(\n",
    "        pl.col('cookie').is_in(df_train['cookie'].unique())\n",
    "        ).filter(\n",
    "            pl.col('node').is_in(df_train['node'].unique())\n",
    "        )\n",
    ")\n",
    "df_eval = df_eval.unique(['cookie', 'node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e6824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.write_parquet(os.path.join(SAVE_PATH, 'clickstream.pq'))\n",
    "df_eval.write_parquet(os.path.join(SAVE_PATH, 'gt.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbcf9",
   "metadata": {},
   "source": [
    "# Retrieval Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3c216",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d7c8d",
   "metadata": {},
   "source": [
    "### EASE_DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e337da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.model import EASE_DAN\n",
    "from utils import Enc, convert_to_sparse, process_in_batches, recall_at\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_PATH = os.path.join(DATA_DIR, 'val')\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'EASE_DAN'\n",
    "EVAL_DAYS_TRESHOLD = 14\n",
    "N_ITEMS = 30_000\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(VAL_PATH, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(VAL_PATH, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq')) \n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d8b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSEEN-RECALL@40 0.17348943701276365\n",
      "UNSEEN-RECALL@100 0.2819826797115451\n"
     ]
    }
   ],
   "source": [
    "enc = Enc(item_key='node', user_key='cookie')\n",
    "enc_eval_users = [enc.user_id_dict.get(i) for i in eval_users]\n",
    "\n",
    "df_train = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(N_ITEMS).drop('len'),\n",
    "                    on='node')\n",
    "\n",
    "\n",
    "df_train = df_train.with_columns(\n",
    "        (pl.lit(1)).alias(\"event_weight\")\n",
    "    )\n",
    "df_eval = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval = df_eval.with_columns(pl.col('node').cast(pl.Int64))\n",
    "result = enc.fit(train_df=df_train, event_weight='event_weight')\n",
    "\n",
    "\n",
    "X = (convert_to_sparse(result, enc) > 0).astype(np.float32)\n",
    "ease = EASE_DAN(num_items=N_ITEMS)\n",
    "ease.fit(X)\n",
    "recommendations_df = process_in_batches(\n",
    "    enc_eval_users=enc_eval_users,\n",
    "    X=X,\n",
    "    G=ease.W,\n",
    "    k=300, # top_k\n",
    "    batch_size=1000,\n",
    "    fill_value=-1000\n",
    ")\n",
    "recs = enc.inverse_transform(recommendations_df) \n",
    "recs = recs.with_columns(\n",
    "    pl.col('score').rank(descending=True).over('cookie').alias(f'rank_rd'),\n",
    "    pl.col('cookie').cast(pl.Int64),\n",
    "    pl.col('node').cast(pl.Int64)\n",
    ")\n",
    "print('UNSEEN-RECALL@40', recall_at(df_eval, recs, k=40))\n",
    "print('UNSEEN-RECALL@100', recall_at(df_eval, recs, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92405b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recs.write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'EASE_DAN_val.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68ff02",
   "metadata": {},
   "source": [
    "### RDLAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.model import RDLAE\n",
    "from utils import truncate, process_batch_w_weight\n",
    "from utils import Enc, convert_to_sparse, process_in_batches, recall_at\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import convert\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_PATH = os.path.join(DATA_DIR, 'val')\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'RDLAE'\n",
    "N_ITEMS = 50_000\n",
    "DECAY_RATE_POS = 0.01\n",
    "DECAY_RATE_TIME = 0.05\n",
    "BAYESSIAN_C = 100\n",
    "SMOOTHED_ALPHA = 1\n",
    "SMOOTHED_BETA = 2\n",
    "NOISE_INJECTION = 0.2\n",
    "RATIO_COLUMN = 'bayesian_ratio_C'\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(VAL_PATH, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(VAL_PATH, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq')) \n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream.join(df_event, on='event', how='left')\n",
    "df_train = df_train.join(df_train.filter(pl.col('is_contact')==1).unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(N_ITEMS).drop('len'),\n",
    "                            on='node')\n",
    "eval_users = df_eval['cookie'].unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6eacc",
   "metadata": {},
   "source": [
    "#### Making bayessian columns to boost contact ration info into collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11141c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = (df_train.select('cookie', 'node', 'event_date', 'is_contact')\n",
    "                           .with_columns(pl.col('event_date').dt.truncate(\"1d\").alias('date'))\n",
    "                           .group_by('cookie', 'node').agg(pl.col('event_date').max().alias('node_last_visit'),\n",
    "                                                           pl.col('event_date').min().alias('node_first_visit'),\n",
    "                                                           (pl.col('date').n_unique()-1).alias('n_days_clicks'),\n",
    "                                                           pl.col('date').filter(pl.col('is_contact') > 0).n_unique().alias('n_days_contacts'),\n",
    "                                                           pl.col('is_contact').sum().cast(pl.Int32),\n",
    "                                                           pl.len().alias('cnt'),\n",
    "                                                           ))\n",
    "time_diff = time_diff.with_columns(\n",
    "    ( -DECAY_RATE_POS * (pl.col('node_last_visit').rank(descending=True).over('cookie')-1)).exp().cast(pl.Float32).alias(f'exp_pos'),\n",
    "    ( DECAY_RATE_TIME * ( pl.col('node_last_visit') - df_train.select('event_date').max() ).dt.total_days().cast(pl.Int64)).exp().cast(pl.Float32).alias('exp_time')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfde14be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cookie</th><th>node</th><th>n_days_clicks</th><th>n_days_contacts</th><th>is_contact</th><th>cnt</th><th>exp_pos</th><th>exp_time</th><th>le_node</th><th>le_cookie</th></tr><tr><td>i64</td><td>u32</td><td>u32</td><td>u32</td><td>i32</td><td>u32</td><td>f32</td><td>f32</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>262019</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0.103312</td><td>0.740818</td><td>36530</td><td>1</td></tr><tr><td>1</td><td>214338</td><td>1</td><td>0</td><td>0</td><td>4</td><td>0.177284</td><td>0.778801</td><td>28726</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 10)\n",
       "┌────────┬────────┬───────────────┬────────────────┬───┬──────────┬──────────┬─────────┬───────────┐\n",
       "│ cookie ┆ node   ┆ n_days_clicks ┆ n_days_contact ┆ … ┆ exp_pos  ┆ exp_time ┆ le_node ┆ le_cookie │\n",
       "│ ---    ┆ ---    ┆ ---           ┆ s              ┆   ┆ ---      ┆ ---      ┆ ---     ┆ ---       │\n",
       "│ i64    ┆ u32    ┆ u32           ┆ ---            ┆   ┆ f32      ┆ f32      ┆ i64     ┆ i64       │\n",
       "│        ┆        ┆               ┆ u32            ┆   ┆          ┆          ┆         ┆           │\n",
       "╞════════╪════════╪═══════════════╪════════════════╪═══╪══════════╪══════════╪═════════╪═══════════╡\n",
       "│ 1      ┆ 262019 ┆ 0             ┆ 0              ┆ … ┆ 0.103312 ┆ 0.740818 ┆ 36530   ┆ 1         │\n",
       "│ 1      ┆ 214338 ┆ 1             ┆ 0              ┆ … ┆ 0.177284 ┆ 0.778801 ┆ 28726   ┆ 1         │\n",
       "└────────┴────────┴───────────────┴────────────────┴───┴──────────┴──────────┴─────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Enc(item_key='node', user_key='cookie')\n",
    "train = enc.fit(train_df=df_train.with_columns(event_weight=1.), event_weight='event_weight')\n",
    "num_users, num_items = enc.get_num()\n",
    "enc_eval_users = [enc.user_id_dict.get(i) for i in eval_users]\n",
    "\n",
    "X = convert_to_sparse(train, enc)\n",
    "\n",
    "n2n = pl.DataFrame({'node':enc.item_id_dict.keys(),   'le_node':enc.item_id_dict.values()})\n",
    "c2c = pl.DataFrame({'cookie':enc.user_id_dict.keys(), 'le_cookie':enc.user_id_dict.values()})\n",
    "\n",
    "train_sum = time_diff.join(n2n, on='node').join(c2c, on='cookie').drop('node_last_visit', 'node_first_visit')\n",
    "train_sum.filter(pl.col('cookie')==1).tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fa89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_user_features = time_diff.group_by('cookie').agg(pl.col('n_days_clicks').sum().alias('sum_n_days_clicks'),\n",
    "                                 pl.col('n_days_clicks').max().alias('max_n_days_clicks'),\n",
    "                                 pl.col('n_days_contacts').sum().alias('sum_n_days_contacts'),\n",
    "                                 pl.col('n_days_contacts').max().alias('max_n_days_contacts'),\n",
    "                                 pl.col('is_contact').sum().alias('sum_is_contact'),\n",
    "                                (pl.col('is_contact').sum() / pl.col('cnt').sum()).alias('user_contact_ratio'),\n",
    "                                (pl.col('exp_pos') * pl.col('is_contact')).sum().alias('exp_pos_contact'),\n",
    "                                (pl.col('exp_time') * pl.col('is_contact')).sum().alias('exp_time_contact'),\n",
    "                                )\n",
    "extra_user_features = extra_user_features.with_columns(\n",
    "    pl.col('user_contact_ratio').cast(pl.Float32),\n",
    "    pl.col('exp_pos_contact').cast(pl.Float32),\n",
    "    pl.col('exp_time_contact').cast(pl.Float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_9016\\3258995628.py:6: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  default_contact_ratio =(df_train\n"
     ]
    }
   ],
   "source": [
    "default_contact_ratio =(df_train\n",
    "                .select('cookie', 'node', 'is_contact', 'event_date').sort('is_contact')\n",
    "                .unique(subset=['cookie', 'node', 'is_contact'], keep='last')\n",
    "                .select('node', 'is_contact').with_columns(value=pl.lit(1))\n",
    "                .pivot(\n",
    "                    values=\"value\",\n",
    "                    index=\"node\",\n",
    "                    columns=\"is_contact\",\n",
    "                    aggregate_function=\"sum\",\n",
    "                ).fill_null(0)).with_columns(pl.col('0').cast(pl.Int32).alias('node_contacts_0'),\n",
    "                                            pl.col('1').cast(pl.Int32).alias('node_contacts_1'),\n",
    "                                            ).drop('0', '1')\n",
    "\n",
    "train_nodes = train_sum.select('node').unique()\n",
    "node_ratio = default_contact_ratio.join(train_nodes, on='node').with_columns(\n",
    "    bayesian_ratio = (BAYESSIAN_C * (pl.col('node_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('node_contacts_1')) / (BAYESSIAN_C + pl.col('node_contacts_0')),\n",
    "    bayesian_ratio_C = (pl.col('node_contacts_0').mean() * (pl.col('node_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('node_contacts_1')) / (pl.col('node_contacts_0').mean() + pl.col('node_contacts_0')),\n",
    "    smoothed_ratio=(pl.col('node_contacts_1') + SMOOTHED_ALPHA) / (pl.col('node_contacts_0') + SMOOTHED_BETA),\n",
    "    noisy_contacts_1 = (pl.Series(np.random.normal(0, NOISE_INJECTION, len(default_contact_ratio.join(train_nodes, on='node')))) *\\\n",
    "          pl.col('node_contacts_1').sqrt() + pl.col('node_contacts_1')).clip(0).round(),\n",
    ").with_columns(\n",
    "    noisy_bayesian_ratio_C = (pl.col('node_contacts_0').mean() * (pl.col('noisy_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('noisy_contacts_1')) / (pl.col('node_contacts_0').mean() + pl.col('node_contacts_0')),\n",
    "    noisy_bayesian_ratio = (BAYESSIAN_C * (pl.col('noisy_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('noisy_contacts_1')) / (BAYESSIAN_C + pl.col('node_contacts_0')),\n",
    "    noisy_smoothed_ratio = (pl.col('noisy_contacts_1') + SMOOTHED_ALPHA) / (pl.col('node_contacts_0') + SMOOTHED_BETA),\n",
    ")\n",
    "\n",
    "try:\n",
    "    node_ratio = node_ratio.join(pl.DataFrame(\n",
    "        {'node':list(enc.item_id_dict.keys()),\n",
    "        'le_node':list(enc.item_id_dict.values())}),\n",
    "        on='node')\n",
    "except Exception:\n",
    "    node_ratioc = node_ratio.drop('le_node').join(pl.DataFrame(\n",
    "        {'node':list(enc.item_id_dict.keys()),\n",
    "        'le_node':list(enc.item_id_dict.values())}),\n",
    "        on='node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30deadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = train_sum.join(node_ratio.select('le_node', RATIO_COLUMN).rename({RATIO_COLUMN:'ratio_column'}) , on='le_node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b362d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = ['cnt','is_contact', 'n_days_clicks','n_days_contacts', 'exp_pos', 'exp_time', 'ratio_column']\n",
    "\n",
    "x_dict = {}\n",
    "for feature in x_features:\n",
    "    x_dict[feature] = convert(train_sum, col=feature, enc=enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8f73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdlae = RDLAE()\n",
    "rdlae.fit((X>0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae30f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = node_ratio.select(RATIO_COLUMN).to_numpy().reshape(-1)\n",
    "weights = np.log1p(weights).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0634277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = truncate(rdlae.G.T, k=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e518c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = process_batch_w_weight(\n",
    "    enc_eval_users=enc_eval_users,\n",
    "    G = (rdlae.G),\n",
    "    X = ((X>0) + 2*(x_dict['is_contact']>0)).astype(np.float32),\n",
    "    Gt = Gt, # доп фичи?\n",
    "    features_dict=x_dict,\n",
    "    weights=(weights).astype(np.float32),\n",
    "    k=300,\n",
    "    batch_size=10_000,\n",
    "    fill_value=-1000 ,\n",
    "    use_torch = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5296675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (16_508_400, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cookie</th><th>node</th><th>score</th></tr><tr><td>i32</td><td>i64</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>1923</td><td>0.050749</td></tr><tr><td>0</td><td>2650</td><td>0.049023</td></tr><tr><td>0</td><td>116118</td><td>0.041513</td></tr><tr><td>0</td><td>187851</td><td>0.028902</td></tr><tr><td>0</td><td>214199</td><td>0.02602</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>149998</td><td>5799</td><td>0.001721</td></tr><tr><td>149998</td><td>17188</td><td>0.0017</td></tr><tr><td>149998</td><td>152032</td><td>0.001697</td></tr><tr><td>149998</td><td>122277</td><td>0.001696</td></tr><tr><td>149998</td><td>153018</td><td>0.001694</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (16_508_400, 3)\n",
       "┌────────┬────────┬──────────┐\n",
       "│ cookie ┆ node   ┆ score    │\n",
       "│ ---    ┆ ---    ┆ ---      │\n",
       "│ i32    ┆ i64    ┆ f32      │\n",
       "╞════════╪════════╪══════════╡\n",
       "│ 0      ┆ 1923   ┆ 0.050749 │\n",
       "│ 0      ┆ 2650   ┆ 0.049023 │\n",
       "│ 0      ┆ 116118 ┆ 0.041513 │\n",
       "│ 0      ┆ 187851 ┆ 0.028902 │\n",
       "│ 0      ┆ 214199 ┆ 0.02602  │\n",
       "│ …      ┆ …      ┆ …        │\n",
       "│ 149998 ┆ 5799   ┆ 0.001721 │\n",
       "│ 149998 ┆ 17188  ┆ 0.0017   │\n",
       "│ 149998 ┆ 152032 ┆ 0.001697 │\n",
       "│ 149998 ┆ 122277 ┆ 0.001696 │\n",
       "│ 149998 ┆ 153018 ┆ 0.001694 │\n",
       "└────────┴────────┴──────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdlae_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3ea78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSEEN-RECALL@40 0.19937169213163403\n",
      "UNSEEN-RECALL@100 0.31355099952500437\n"
     ]
    }
   ],
   "source": [
    "rdlae_recs = enc.inverse_transform(recommendations_df) \n",
    "print('UNSEEN-RECALL@40', recall_at(df_eval, rdlae_recs.with_columns(pl.col('cookie').cast(pl.Int64), pl.col('node').cast(pl.UInt32)), k=40))\n",
    "print('UNSEEN-RECALL@100', recall_at(df_eval, rdlae_recs.with_columns(pl.col('cookie').cast(pl.Int64), pl.col('node').cast(pl.UInt32)), k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92d889ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "rdlae_recs.write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, '{MODEL_NAME}_val.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34791e",
   "metadata": {},
   "source": [
    "## SasRec Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15f956",
   "metadata": {},
   "source": [
    "### SasRec over node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f204f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from replay.metrics import OfflineMetrics, Recall, Precision, MAP, NDCG, HitRate, MRR\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.splitters import LastNSplitter\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory\n",
    "from replay.models.nn.sequential.callbacks import (\n",
    "    ValidationMetricsCallback,\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback,\n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback,\n",
    ")\n",
    "from replay.models.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.data.nn import SequenceTokenizer, SequentialDataset, TensorFeatureSource, TensorSchema, TensorFeatureInfo\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionDataset,\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecModel,\n",
    ")\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_DIR = 'val'\n",
    "\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()\n",
    "\n",
    "n_nodes = 30_000\n",
    "train_small = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(n_nodes).drop('len'),\n",
    "                        on='node')\n",
    "df_eval_small = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval_small = df_eval_small.with_columns(pl.col('node').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848d9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 30000\n",
      "After: 30000\n",
      "Items save 100.0 %\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    all_features = base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def filter_n_count(df, column='item_id', n=5):\n",
    "    to_save = df.group_by(column).agg(pl.count().alias('count')).filter(pl.col('count')>n)\n",
    "    return df.join(to_save, on=column, how='semi')\n",
    "\n",
    "def allign_gt(df, gt, item_col='item_id', user_col='user_id'):\n",
    "    gt = gt.join(df, on=user_col, how='semi')\n",
    "    gt = gt.join(df, on=item_col, how='semi')\n",
    "    return gt\n",
    "\n",
    "def make_replay_format(df, is_gt=False):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col('cookie').alias('user_id').cast(pl.Int64), \n",
    "            pl.col('node').alias('item_id').cast(pl.Int64)\n",
    "        ])\n",
    "    if not is_gt:\n",
    "        df = df.with_columns(pl.col(\"event_date\").dt.timestamp(\"ms\").alias(\"timestamp\") // 1000)\n",
    "    return df\n",
    "\n",
    "validation_gt = make_replay_format(df_eval_small, is_gt=True)['user_id', 'item_id']\n",
    "train_events = make_replay_format(train_small)['user_id', 'item_id','timestamp']\n",
    "item_before = train_events['item_id'].unique().len()\n",
    "item_after = train_events['item_id'].unique().len()\n",
    "\n",
    "print(f'Before: {item_before}')\n",
    "print(f'After: {item_after}')\n",
    "print(f'Items save {100 * item_after/item_before} %')\n",
    "validation_gt = allign_gt(train_events, validation_gt)\n",
    "user_features = train_events[['user_id']].unique()\n",
    "item_features = train_events[['item_id']].unique()\n",
    "validation_events = train_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8991d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\2237632128.py:79: DeprecationWarning: SasRecTrainingDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecTrainingDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:79: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\2237632128.py:90: DeprecationWarning: SasRecValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:206: DeprecationWarning: TorchSequentialValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\data\\nn\\torch_sequential_dataset.py:235: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\code\\avito_hack_clear\\.checkpoints\\sasrec_replay exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type             | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | _model | SasRecModel      | 4.1 M  | train\n",
      "1 | _loss  | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.209    Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf348df72af4afe9294d645207e703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de412c6037f14e02809179b5d3f9ae51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5c6937d03846cc8c1f0f674e10fae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 989: 'recall@40' reached 0.14474 (best 0.14474), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=0-step=989.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.234696  0.097424  0.144743\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c8f4c94074ec39899be84fc3bc9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1978: 'recall@40' reached 0.15222 (best 0.15222), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=1-step=1978.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.246902  0.101545  0.152224\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ed20e3b594425a5132b41065beea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2967: 'recall@40' reached 0.15333 (best 0.15333), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=2-step=2967.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.248557  0.102813  0.153327\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9a3e345f3040feb8b1e9e21fb2e8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3956: 'recall@40' reached 0.15561 (best 0.15561), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=3-step=3956-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100       20       40\n",
      "recall  0.254591  0.10464  0.15561\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2547c3bdecfe46b589d25317af6b86ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4945: 'recall@40' reached 0.15818 (best 0.15818), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=4-step=4945.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.258511  0.104967  0.158176\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6bb0fa8473485eb32933118d79591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 5934: 'recall@40' reached 0.16291 (best 0.16291), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=5-step=5934.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.266703  0.108921  0.162912\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b3e7c001804dce87a4bc61fbf4fbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 6923: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k           100        20        40\n",
      "recall  0.26048  0.104784  0.158601\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb334921ee94afb964d7ac91c57e813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 7912: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.263805  0.107246  0.161621\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e88581ff3a4f0cb90b17487cd08371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 8901: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100       20        40\n",
      "recall  0.264808  0.10683  0.161539\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c294c87ec34ed58fa09eb54e2cd8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 9890: 'recall@40' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.261147  0.105352  0.159423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 9\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=ITEM_FEATURE_NAME,\n",
    "            is_seq=True,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column)],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "sequential_train_dataset = tokenizer.transform(train_dataset)\n",
    "\n",
    "sequential_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "sequential_validation_gt = tokenizer.transform(validation_gt, [tensor_schema.item_id_feature_name])\n",
    "\n",
    "sequential_validation_dataset, sequential_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "    sequential_validation_dataset, sequential_validation_gt\n",
    ")\n",
    "\n",
    "model = SasRec(\n",
    "    tensor_schema,\n",
    "    block_count=2,\n",
    "    head_count=2,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    optimizer_factory=FatOptimizerFactory(learning_rate=0.001),\n",
    "    loss_sample_count=4_000,\n",
    "    negatives_sharing=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SASRec\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints/sasrec_replay\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@40\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=SasRecValidationDataset(\n",
    "        sequential_validation_dataset,\n",
    "        sequential_validation_gt,\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"recall\", \n",
    "             # \"ndcg\", \"map\", \"coverage\"\n",
    "             ],\n",
    "    ks=[20, 40, 100],\n",
    "    item_count=train_dataset.item_count,\n",
    "    postprocessors=[RemoveSeenItems(sequential_validation_dataset)],\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    "    accelerator='cuda',\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=validation_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6b6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\582167438.py:7: DeprecationWarning: SasRecPredictionDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecPredictionDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:146: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbebaceea99c44c0a8e59fd1b8526e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'SASREC_REPLAY_NODE'\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=SasRecPredictionDataset(\n",
    "        sequential_validation_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SASRec\")\n",
    "\n",
    "TOPK = [300]\n",
    "\n",
    "postprocessors = [RemoveSeenItems(sequential_validation_dataset)]\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        pandas_prediction_callback,\n",
    "    ],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True,\n",
    ")\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(pandas_res)\n",
    "recommendations = pl.from_pandas(recommendations)\n",
    "\n",
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recommendations.rename({'user_id':'cookie', 'item_id':'node'}).write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'SASREC_NODE_REPLAY_VAL.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65381d39",
   "metadata": {},
   "source": [
    "### SasRec Replay over category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd760420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import lightning as L\n",
    "import os\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from replay.metrics import OfflineMetrics, Recall, Precision, MAP, NDCG, HitRate, MRR\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.splitters import LastNSplitter\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory\n",
    "from replay.models.nn.sequential.callbacks import (\n",
    "    ValidationMetricsCallback,\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback,\n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback,\n",
    ")\n",
    "from replay.models.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.data.nn import SequenceTokenizer, SequentialDataset, TensorFeatureSource, TensorSchema, TensorFeatureInfo\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionDataset,\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecModel,\n",
    ")\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_DIR = 'val'\n",
    "\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()\n",
    "\n",
    "n_nodes = 30_000\n",
    "train_small = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(n_nodes).drop('len'),\n",
    "                        on='node')\n",
    "df_eval_small = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval_small = df_eval_small.with_columns(pl.col('node').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02df29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 51\n",
      "After: 51\n",
      "Items save 100.0 %\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    all_features = base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def filter_n_count(df, column='item_id', n=5):\n",
    "    to_save = df.group_by(column).agg(pl.count().alias('count')).filter(pl.col('count')>n)\n",
    "    return df.join(to_save, on=column, how='semi')\n",
    "\n",
    "def allign_gt(df, gt, item_col='item_id', user_col='user_id'):\n",
    "    gt = gt.join(df, on=user_col, how='semi')\n",
    "    gt = gt.join(df, on=item_col, how='semi')\n",
    "    return gt\n",
    "\n",
    "def make_replay_format(df, is_gt=False):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col('cookie').alias('user_id').cast(pl.Int64), \n",
    "            pl.col('category').alias('item_id').cast(pl.Int64)\n",
    "        ])\n",
    "    if not is_gt:\n",
    "        df = df.with_columns(pl.col(\"event_date\").dt.timestamp(\"ms\").alias(\"timestamp\") // 1000)\n",
    "    return df\n",
    "\n",
    "validation_gt = make_replay_format(df_eval_small.join(df_cat_features.with_columns(pl.col('node').cast(pl.Int64)).drop_nulls().select('node', 'category').unique(), on='node', how='left'), is_gt=True)['user_id', 'item_id'].unique()\n",
    "train_events = make_replay_format(train_small.join(df_cat_features.drop_nulls().select('node', 'category').unique(), on='node', how='left'))['user_id', 'item_id','timestamp']\n",
    "item_before = train_events['item_id'].unique().len()\n",
    "item_after = train_events['item_id'].unique().len()\n",
    "\n",
    "print(f'Before: {item_before}')\n",
    "print(f'After: {item_after}')\n",
    "print(f'Items save {100 * item_after/item_before} %')\n",
    "validation_gt = allign_gt(train_events, validation_gt)\n",
    "item_features = train_events[['item_id']].unique()\n",
    "validation_events = train_events\n",
    "\n",
    "user_features = make_replay_format(df_clickstream.join(df_cat_features.drop_nulls().select('node', 'category').unique(), on='node', how='left'))[['user_id']].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fe870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\3415952068.py:79: DeprecationWarning: SasRecTrainingDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecTrainingDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:79: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\3415952068.py:90: DeprecationWarning: SasRecValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:206: DeprecationWarning: TorchSequentialValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\data\\nn\\torch_sequential_dataset.py:235: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\code\\avito_hack_clear\\.checkpoints\\sasrec_category exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type             | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | _model | SasRecModel      | 218 K  | train\n",
      "1 | _loss  | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "218 K     Trainable params\n",
      "0         Non-trainable params\n",
      "218 K     Total params\n",
      "0.876     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c4825dff464232ab2796be434d7c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f49b73a841438cb13c2e16a2ea272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3d6d793d9c4d608f2cc5a135312690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 248: 'ndcg@5' reached 0.46160 (best 0.46160), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_category\\\\epoch=0-step=248-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.298366  0.411104  0.381566\n",
      "ndcg    0.298366  0.518872  0.461601\n",
      "recall  0.212957  0.765572  0.609367\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0336f42d9344dc2a9096e7603bd1231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 496: 'ndcg@5' reached 0.46626 (best 0.46626), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_category\\\\epoch=1-step=496.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.304509  0.415716  0.386477\n",
      "ndcg    0.304509  0.522943  0.466262\n",
      "recall  0.217589  0.767836  0.613223\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d2461321cd44009600a4bf4fdadf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 744: 'ndcg@5' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.304745  0.415012  0.385685\n",
      "ndcg    0.304745  0.522484  0.465802\n",
      "recall  0.217629  0.768077  0.613735\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cd275f31864dff85ba725d2b079bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 992: 'ndcg@5' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.301401  0.412559  0.382953\n",
      "ndcg    0.301401  0.520390  0.463110\n",
      "recall  0.214911  0.767287  0.611289\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf98c7477064cfa926f647890e4aeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1240: 'ndcg@5' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.298312  0.410138  0.380102\n",
      "ndcg    0.298312  0.518359  0.460306\n",
      "recall  0.212801  0.766803  0.608767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 9\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=ITEM_FEATURE_NAME,\n",
    "            is_seq=True,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column)],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "sequential_train_dataset = tokenizer.transform(train_dataset)\n",
    "\n",
    "sequential_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "sequential_validation_gt = tokenizer.transform(validation_gt, [tensor_schema.item_id_feature_name])\n",
    "\n",
    "sequential_validation_dataset, sequential_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "    sequential_validation_dataset, sequential_validation_gt\n",
    ")\n",
    "\n",
    "model = SasRec(\n",
    "    tensor_schema,\n",
    "    block_count=2,\n",
    "    head_count=2,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    optimizer_factory=FatOptimizerFactory(learning_rate=0.001),\n",
    "    loss_sample_count=50,\n",
    "    negatives_sharing=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SASRec_category\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints/sasrec_category\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"ndcg@5\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=SasRecValidationDataset(\n",
    "        sequential_validation_dataset,\n",
    "        sequential_validation_gt,\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"recall\", 'map', 'ndcg'],\n",
    "    ks=[1, 5, 10],\n",
    "    item_count=train_dataset.item_count,\n",
    "    postprocessors=[],\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    "    accelerator='cuda',\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=validation_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0dfe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\2460397807.py:7: DeprecationWarning: SasRecPredictionDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecPredictionDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:146: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b5c233c739416881f30a21598c7941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'SASREC_REPLAY_CATEGORY'\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=SasRecPredictionDataset(\n",
    "        sequential_validation_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SASRec_category\")\n",
    "\n",
    "TOPK = [50]\n",
    "\n",
    "postprocessors = []\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        pandas_prediction_callback,\n",
    "    ],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True,\n",
    ")\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(pandas_res)\n",
    "recommendations = pl.from_pandas(recommendations)\n",
    "\n",
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recommendations.rename({'user_id':'cookie', 'item_id':'node'}).write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'SASREC_CATEGORY_REPLAY_VAL.pq'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
