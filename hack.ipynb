{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069d393b",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6969e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "import os\n",
    "DATA_DIR = 'data'\n",
    "SAVE_PATH = os.path.join(DATA_DIR, 'val')\n",
    "os.makedirs(os.path.join(DATA_DIR, 'val'), exist_ok=True)\n",
    "EVAL_DAYS_TRESHOLD = 14\n",
    "\n",
    "df_test_users = pl.read_parquet(os.path.join(DATA_DIR, 'test_users.pq'))\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, 'clickstream.pq'))\n",
    "\n",
    "#df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "#df_text_features = pl.read_parquet(os.path.join(DATA_DIR, 'clickstream.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f167539",
   "metadata": {},
   "outputs": [],
   "source": [
    "treshhold = df_clickstream['event_date'].max() - timedelta(days=EVAL_DAYS_TRESHOLD)\n",
    "\n",
    "df_train = df_clickstream.filter(df_clickstream['event_date']<= treshhold)\n",
    "df_eval = df_clickstream.filter(df_clickstream['event_date']> treshhold)[['cookie', 'node', 'event']]\n",
    "\n",
    "df_eval = (\n",
    "        df_eval\n",
    "        .join(df_train, on=['cookie', 'node'], how='anti')\n",
    "        .filter(\n",
    "                pl.col('event').is_in(\n",
    "                    df_event.filter(pl.col('is_contact')==1)['event'].unique()\n",
    "                )\n",
    "            )\n",
    "        .filter(\n",
    "        pl.col('cookie').is_in(df_train['cookie'].unique())\n",
    "        ).filter(\n",
    "            pl.col('node').is_in(df_train['node'].unique())\n",
    "        )\n",
    ")\n",
    "df_eval = df_eval.unique(['cookie', 'node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e6824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.write_parquet(os.path.join(SAVE_PATH, 'clickstream.pq'))\n",
    "df_eval.write_parquet(os.path.join(SAVE_PATH, 'gt.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbcf9",
   "metadata": {},
   "source": [
    "# Retrieval Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3c216",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d7c8d",
   "metadata": {},
   "source": [
    "### EASE_DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e337da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.model import EASE_DAN\n",
    "from utils import Enc, convert_to_sparse, process_in_batches, recall_at\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_PATH = os.path.join(DATA_DIR, 'val')\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'EASE_DAN'\n",
    "EVAL_DAYS_TRESHOLD = 14\n",
    "N_ITEMS = 30_000\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(VAL_PATH, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(VAL_PATH, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq')) \n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d8b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSEEN-RECALL@40 0.17348943701276365\n",
      "UNSEEN-RECALL@100 0.2819826797115451\n"
     ]
    }
   ],
   "source": [
    "enc = Enc(item_key='node', user_key='cookie')\n",
    "enc_eval_users = [enc.user_id_dict.get(i) for i in eval_users]\n",
    "\n",
    "df_train = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(N_ITEMS).drop('len'),\n",
    "                    on='node')\n",
    "\n",
    "\n",
    "df_train = df_train.with_columns(\n",
    "        (pl.lit(1)).alias(\"event_weight\")\n",
    "    )\n",
    "df_eval = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval = df_eval.with_columns(pl.col('node').cast(pl.Int64))\n",
    "result = enc.fit(train_df=df_train, event_weight='event_weight')\n",
    "\n",
    "\n",
    "X = (convert_to_sparse(result, enc) > 0).astype(np.float32)\n",
    "ease = EASE_DAN(num_items=N_ITEMS)\n",
    "ease.fit(X)\n",
    "recommendations_df = process_in_batches(\n",
    "    enc_eval_users=enc_eval_users,\n",
    "    X=X,\n",
    "    G=ease.W,\n",
    "    k=300, # top_k\n",
    "    batch_size=1000,\n",
    "    fill_value=-1000\n",
    ")\n",
    "recs = enc.inverse_transform(recommendations_df) \n",
    "recs = recs.with_columns(\n",
    "    pl.col('score').rank(descending=True).over('cookie').alias(f'rank_rd'),\n",
    "    pl.col('cookie').cast(pl.Int64),\n",
    "    pl.col('node').cast(pl.Int64)\n",
    ")\n",
    "print('UNSEEN-RECALL@40', recall_at(df_eval, recs, k=40))\n",
    "print('UNSEEN-RECALL@100', recall_at(df_eval, recs, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92405b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recs.write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'EASE_DAN_VAL.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68ff02",
   "metadata": {},
   "source": [
    "### RDLAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.model import RDLAE\n",
    "from utils import truncate, process_batch_w_weight\n",
    "from utils import Enc, convert_to_sparse, process_in_batches, recall_at\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import convert\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_PATH = os.path.join(DATA_DIR, 'val')\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'RDLAE'\n",
    "N_ITEMS = 50_000\n",
    "DECAY_RATE_POS = 0.01\n",
    "DECAY_RATE_TIME = 0.05\n",
    "BAYESSIAN_C = 100\n",
    "SMOOTHED_ALPHA = 1\n",
    "SMOOTHED_BETA = 2\n",
    "NOISE_INJECTION = 0.2\n",
    "RATIO_COLUMN = 'bayesian_ratio_C'\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(VAL_PATH, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(VAL_PATH, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq')) \n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream.join(df_event, on='event', how='left')\n",
    "df_train = df_train.join(df_train.filter(pl.col('is_contact')==1).unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(N_ITEMS).drop('len'),\n",
    "                            on='node')\n",
    "eval_users = df_eval['cookie'].unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6eacc",
   "metadata": {},
   "source": [
    "#### Making bayessian columns to boost contact ration info into collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11141c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = (df_train.select('cookie', 'node', 'event_date', 'is_contact')\n",
    "                           .with_columns(pl.col('event_date').dt.truncate(\"1d\").alias('date'))\n",
    "                           .group_by('cookie', 'node').agg(pl.col('event_date').max().alias('node_last_visit'),\n",
    "                                                           pl.col('event_date').min().alias('node_first_visit'),\n",
    "                                                           (pl.col('date').n_unique()-1).alias('n_days_clicks'),\n",
    "                                                           pl.col('date').filter(pl.col('is_contact') > 0).n_unique().alias('n_days_contacts'),\n",
    "                                                           pl.col('is_contact').sum().cast(pl.Int32),\n",
    "                                                           pl.len().alias('cnt'),\n",
    "                                                           ))\n",
    "time_diff = time_diff.with_columns(\n",
    "    ( -DECAY_RATE_POS * (pl.col('node_last_visit').rank(descending=True).over('cookie')-1)).exp().cast(pl.Float32).alias(f'exp_pos'),\n",
    "    ( DECAY_RATE_TIME * ( pl.col('node_last_visit') - df_train.select('event_date').max() ).dt.total_days().cast(pl.Int64)).exp().cast(pl.Float32).alias('exp_time')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfde14be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cookie</th><th>node</th><th>n_days_clicks</th><th>n_days_contacts</th><th>is_contact</th><th>cnt</th><th>exp_pos</th><th>exp_time</th><th>le_node</th><th>le_cookie</th></tr><tr><td>i64</td><td>u32</td><td>u32</td><td>u32</td><td>i32</td><td>u32</td><td>f32</td><td>f32</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>262019</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0.103312</td><td>0.740818</td><td>36530</td><td>1</td></tr><tr><td>1</td><td>214338</td><td>1</td><td>0</td><td>0</td><td>4</td><td>0.177284</td><td>0.778801</td><td>28726</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 10)\n",
       "┌────────┬────────┬───────────────┬────────────────┬───┬──────────┬──────────┬─────────┬───────────┐\n",
       "│ cookie ┆ node   ┆ n_days_clicks ┆ n_days_contact ┆ … ┆ exp_pos  ┆ exp_time ┆ le_node ┆ le_cookie │\n",
       "│ ---    ┆ ---    ┆ ---           ┆ s              ┆   ┆ ---      ┆ ---      ┆ ---     ┆ ---       │\n",
       "│ i64    ┆ u32    ┆ u32           ┆ ---            ┆   ┆ f32      ┆ f32      ┆ i64     ┆ i64       │\n",
       "│        ┆        ┆               ┆ u32            ┆   ┆          ┆          ┆         ┆           │\n",
       "╞════════╪════════╪═══════════════╪════════════════╪═══╪══════════╪══════════╪═════════╪═══════════╡\n",
       "│ 1      ┆ 262019 ┆ 0             ┆ 0              ┆ … ┆ 0.103312 ┆ 0.740818 ┆ 36530   ┆ 1         │\n",
       "│ 1      ┆ 214338 ┆ 1             ┆ 0              ┆ … ┆ 0.177284 ┆ 0.778801 ┆ 28726   ┆ 1         │\n",
       "└────────┴────────┴───────────────┴────────────────┴───┴──────────┴──────────┴─────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Enc(item_key='node', user_key='cookie')\n",
    "train = enc.fit(train_df=df_train.with_columns(event_weight=1.), event_weight='event_weight')\n",
    "num_users, num_items = enc.get_num()\n",
    "enc_eval_users = [enc.user_id_dict.get(i) for i in eval_users]\n",
    "\n",
    "X = convert_to_sparse(train, enc)\n",
    "\n",
    "n2n = pl.DataFrame({'node':enc.item_id_dict.keys(),   'le_node':enc.item_id_dict.values()})\n",
    "c2c = pl.DataFrame({'cookie':enc.user_id_dict.keys(), 'le_cookie':enc.user_id_dict.values()})\n",
    "\n",
    "train_sum = time_diff.join(n2n, on='node').join(c2c, on='cookie').drop('node_last_visit', 'node_first_visit')\n",
    "train_sum.filter(pl.col('cookie')==1).tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fa89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_user_features = time_diff.group_by('cookie').agg(pl.col('n_days_clicks').sum().alias('sum_n_days_clicks'),\n",
    "                                 pl.col('n_days_clicks').max().alias('max_n_days_clicks'),\n",
    "                                 pl.col('n_days_contacts').sum().alias('sum_n_days_contacts'),\n",
    "                                 pl.col('n_days_contacts').max().alias('max_n_days_contacts'),\n",
    "                                 pl.col('is_contact').sum().alias('sum_is_contact'),\n",
    "                                (pl.col('is_contact').sum() / pl.col('cnt').sum()).alias('user_contact_ratio'),\n",
    "                                (pl.col('exp_pos') * pl.col('is_contact')).sum().alias('exp_pos_contact'),\n",
    "                                (pl.col('exp_time') * pl.col('is_contact')).sum().alias('exp_time_contact'),\n",
    "                                )\n",
    "extra_user_features = extra_user_features.with_columns(\n",
    "    pl.col('user_contact_ratio').cast(pl.Float32),\n",
    "    pl.col('exp_pos_contact').cast(pl.Float32),\n",
    "    pl.col('exp_time_contact').cast(pl.Float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_9016\\3258995628.py:6: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  default_contact_ratio =(df_train\n"
     ]
    }
   ],
   "source": [
    "default_contact_ratio =(df_train\n",
    "                .select('cookie', 'node', 'is_contact', 'event_date').sort('is_contact')\n",
    "                .unique(subset=['cookie', 'node', 'is_contact'], keep='last')\n",
    "                .select('node', 'is_contact').with_columns(value=pl.lit(1))\n",
    "                .pivot(\n",
    "                    values=\"value\",\n",
    "                    index=\"node\",\n",
    "                    columns=\"is_contact\",\n",
    "                    aggregate_function=\"sum\",\n",
    "                ).fill_null(0)).with_columns(pl.col('0').cast(pl.Int32).alias('node_contacts_0'),\n",
    "                                            pl.col('1').cast(pl.Int32).alias('node_contacts_1'),\n",
    "                                            ).drop('0', '1')\n",
    "\n",
    "train_nodes = train_sum.select('node').unique()\n",
    "node_ratio = default_contact_ratio.join(train_nodes, on='node').with_columns(\n",
    "    bayesian_ratio = (BAYESSIAN_C * (pl.col('node_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('node_contacts_1')) / (BAYESSIAN_C + pl.col('node_contacts_0')),\n",
    "    bayesian_ratio_C = (pl.col('node_contacts_0').mean() * (pl.col('node_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('node_contacts_1')) / (pl.col('node_contacts_0').mean() + pl.col('node_contacts_0')),\n",
    "    smoothed_ratio=(pl.col('node_contacts_1') + SMOOTHED_ALPHA) / (pl.col('node_contacts_0') + SMOOTHED_BETA),\n",
    "    noisy_contacts_1 = (pl.Series(np.random.normal(0, NOISE_INJECTION, len(default_contact_ratio.join(train_nodes, on='node')))) *\\\n",
    "          pl.col('node_contacts_1').sqrt() + pl.col('node_contacts_1')).clip(0).round(),\n",
    ").with_columns(\n",
    "    noisy_bayesian_ratio_C = (pl.col('node_contacts_0').mean() * (pl.col('noisy_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('noisy_contacts_1')) / (pl.col('node_contacts_0').mean() + pl.col('node_contacts_0')),\n",
    "    noisy_bayesian_ratio = (BAYESSIAN_C * (pl.col('noisy_contacts_1').sum() / pl.col('node_contacts_0').sum()) + pl.col('noisy_contacts_1')) / (BAYESSIAN_C + pl.col('node_contacts_0')),\n",
    "    noisy_smoothed_ratio = (pl.col('noisy_contacts_1') + SMOOTHED_ALPHA) / (pl.col('node_contacts_0') + SMOOTHED_BETA),\n",
    ")\n",
    "\n",
    "try:\n",
    "    node_ratio = node_ratio.join(pl.DataFrame(\n",
    "        {'node':list(enc.item_id_dict.keys()),\n",
    "        'le_node':list(enc.item_id_dict.values())}),\n",
    "        on='node')\n",
    "except Exception:\n",
    "    node_ratioc = node_ratio.drop('le_node').join(pl.DataFrame(\n",
    "        {'node':list(enc.item_id_dict.keys()),\n",
    "        'le_node':list(enc.item_id_dict.values())}),\n",
    "        on='node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30deadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sum = train_sum.join(node_ratio.select('le_node', RATIO_COLUMN).rename({RATIO_COLUMN:'ratio_column'}) , on='le_node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b362d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = ['cnt','is_contact', 'n_days_clicks','n_days_contacts', 'exp_pos', 'exp_time', 'ratio_column']\n",
    "\n",
    "x_dict = {}\n",
    "for feature in x_features:\n",
    "    x_dict[feature] = convert(train_sum, col=feature, enc=enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8f73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdlae = RDLAE()\n",
    "rdlae.fit((X>0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae30f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = node_ratio.select(RATIO_COLUMN).to_numpy().reshape(-1)\n",
    "weights = np.log1p(weights).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0634277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = truncate(rdlae.G.T, k=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = process_batch_w_weight(\n",
    "    enc_eval_users=enc_eval_users,\n",
    "    G = (rdlae.G),\n",
    "    X = ((X>0) + 2*(x_dict['is_contact']>0)).astype(np.float32),\n",
    "    Gt = Gt,\n",
    "    features_dict=x_dict,\n",
    "    weights=(weights).astype(np.float32),\n",
    "    k=300,\n",
    "    batch_size=10_000,\n",
    "    fill_value=-1000 ,\n",
    "    use_torch = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3ea78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSEEN-RECALL@40 0.19937169213163403\n",
      "UNSEEN-RECALL@100 0.31355099952500437\n"
     ]
    }
   ],
   "source": [
    "rdlae_recs = enc.inverse_transform(recommendations_df) \n",
    "print('UNSEEN-RECALL@40', recall_at(df_eval, rdlae_recs.with_columns(pl.col('cookie').cast(pl.Int64), pl.col('node').cast(pl.UInt32)), k=40))\n",
    "print('UNSEEN-RECALL@100', recall_at(df_eval, rdlae_recs.with_columns(pl.col('cookie').cast(pl.Int64), pl.col('node').cast(pl.UInt32)), k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d889ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "rdlae_recs.write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, f'{MODEL_NAME}_VAL.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34791e",
   "metadata": {},
   "source": [
    "## SasRec Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15f956",
   "metadata": {},
   "source": [
    "### SasRec over node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f204f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from replay.metrics import OfflineMetrics, Recall, Precision, MAP, NDCG, HitRate, MRR\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.splitters import LastNSplitter\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory\n",
    "from replay.models.nn.sequential.callbacks import (\n",
    "    ValidationMetricsCallback,\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback,\n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback,\n",
    ")\n",
    "from replay.models.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.data.nn import SequenceTokenizer, SequentialDataset, TensorFeatureSource, TensorSchema, TensorFeatureInfo\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionDataset,\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecModel,\n",
    ")\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_DIR = 'val'\n",
    "\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()\n",
    "\n",
    "n_nodes = 30_000\n",
    "train_small = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(n_nodes).drop('len'),\n",
    "                        on='node')\n",
    "df_eval_small = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval_small = df_eval_small.with_columns(pl.col('node').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848d9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 30000\n",
      "After: 30000\n",
      "Items save 100.0 %\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    all_features = base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def filter_n_count(df, column='item_id', n=5):\n",
    "    to_save = df.group_by(column).agg(pl.count().alias('count')).filter(pl.col('count')>n)\n",
    "    return df.join(to_save, on=column, how='semi')\n",
    "\n",
    "def allign_gt(df, gt, item_col='item_id', user_col='user_id'):\n",
    "    gt = gt.join(df, on=user_col, how='semi')\n",
    "    gt = gt.join(df, on=item_col, how='semi')\n",
    "    return gt\n",
    "\n",
    "def make_replay_format(df, is_gt=False):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col('cookie').alias('user_id').cast(pl.Int64), \n",
    "            pl.col('node').alias('item_id').cast(pl.Int64)\n",
    "        ])\n",
    "    if not is_gt:\n",
    "        df = df.with_columns(pl.col(\"event_date\").dt.timestamp(\"ms\").alias(\"timestamp\") // 1000)\n",
    "    return df\n",
    "\n",
    "validation_gt = make_replay_format(df_eval_small, is_gt=True)['user_id', 'item_id']\n",
    "train_events = make_replay_format(train_small)['user_id', 'item_id','timestamp']\n",
    "item_before = train_events['item_id'].unique().len()\n",
    "item_after = train_events['item_id'].unique().len()\n",
    "\n",
    "print(f'Before: {item_before}')\n",
    "print(f'After: {item_after}')\n",
    "print(f'Items save {100 * item_after/item_before} %')\n",
    "validation_gt = allign_gt(train_events, validation_gt)\n",
    "user_features = train_events[['user_id']].unique()\n",
    "item_features = train_events[['item_id']].unique()\n",
    "validation_events = train_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8991d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\2237632128.py:79: DeprecationWarning: SasRecTrainingDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecTrainingDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:79: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\2237632128.py:90: DeprecationWarning: SasRecValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:206: DeprecationWarning: TorchSequentialValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\data\\nn\\torch_sequential_dataset.py:235: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\code\\avito_hack_clear\\.checkpoints\\sasrec_replay exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type             | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | _model | SasRecModel      | 4.1 M  | train\n",
      "1 | _loss  | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.209    Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf348df72af4afe9294d645207e703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de412c6037f14e02809179b5d3f9ae51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5c6937d03846cc8c1f0f674e10fae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 989: 'recall@40' reached 0.14474 (best 0.14474), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=0-step=989.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.234696  0.097424  0.144743\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c8f4c94074ec39899be84fc3bc9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1978: 'recall@40' reached 0.15222 (best 0.15222), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=1-step=1978.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.246902  0.101545  0.152224\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ed20e3b594425a5132b41065beea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2967: 'recall@40' reached 0.15333 (best 0.15333), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=2-step=2967.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.248557  0.102813  0.153327\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9a3e345f3040feb8b1e9e21fb2e8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3956: 'recall@40' reached 0.15561 (best 0.15561), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=3-step=3956-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100       20       40\n",
      "recall  0.254591  0.10464  0.15561\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2547c3bdecfe46b589d25317af6b86ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4945: 'recall@40' reached 0.15818 (best 0.15818), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=4-step=4945.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.258511  0.104967  0.158176\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6bb0fa8473485eb32933118d79591f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 5934: 'recall@40' reached 0.16291 (best 0.16291), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_replay\\\\epoch=5-step=5934.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.266703  0.108921  0.162912\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b3e7c001804dce87a4bc61fbf4fbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 6923: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k           100        20        40\n",
      "recall  0.26048  0.104784  0.158601\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb334921ee94afb964d7ac91c57e813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 7912: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.263805  0.107246  0.161621\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e88581ff3a4f0cb90b17487cd08371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 8901: 'recall@40' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100       20        40\n",
      "recall  0.264808  0.10683  0.161539\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c294c87ec34ed58fa09eb54e2cd8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 9890: 'recall@40' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            100        20        40\n",
      "recall  0.261147  0.105352  0.159423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 9\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=ITEM_FEATURE_NAME,\n",
    "            is_seq=True,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column)],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "sequential_train_dataset = tokenizer.transform(train_dataset)\n",
    "\n",
    "sequential_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "sequential_validation_gt = tokenizer.transform(validation_gt, [tensor_schema.item_id_feature_name])\n",
    "\n",
    "sequential_validation_dataset, sequential_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "    sequential_validation_dataset, sequential_validation_gt\n",
    ")\n",
    "\n",
    "model = SasRec(\n",
    "    tensor_schema,\n",
    "    block_count=2,\n",
    "    head_count=2,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    optimizer_factory=FatOptimizerFactory(learning_rate=0.001),\n",
    "    loss_sample_count=4_000,\n",
    "    negatives_sharing=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SASRec\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints/sasrec_replay\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@40\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=SasRecValidationDataset(\n",
    "        sequential_validation_dataset,\n",
    "        sequential_validation_gt,\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"recall\", \n",
    "             # \"ndcg\", \"map\", \"coverage\"\n",
    "             ],\n",
    "    ks=[20, 40, 100],\n",
    "    item_count=train_dataset.item_count,\n",
    "    postprocessors=[RemoveSeenItems(sequential_validation_dataset)],\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    "    accelerator='cuda',\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=validation_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6b6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_8456\\582167438.py:7: DeprecationWarning: SasRecPredictionDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecPredictionDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:146: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbebaceea99c44c0a8e59fd1b8526e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'SASREC_REPLAY_NODE'\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=SasRecPredictionDataset(\n",
    "        sequential_validation_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SASRec\")\n",
    "\n",
    "TOPK = [300]\n",
    "\n",
    "postprocessors = [RemoveSeenItems(sequential_validation_dataset)]\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        pandas_prediction_callback,\n",
    "    ],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True,\n",
    ")\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(pandas_res)\n",
    "recommendations = pl.from_pandas(recommendations)\n",
    "\n",
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recommendations.rename({'user_id':'cookie', 'item_id':'node'}).write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'SASREC_NODE_REPLAY_VAL.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65381d39",
   "metadata": {},
   "source": [
    "### SasRec Replay over category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd760420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import lightning as L\n",
    "import os\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from replay.metrics import OfflineMetrics, Recall, Precision, MAP, NDCG, HitRate, MRR\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.splitters import LastNSplitter\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory\n",
    "from replay.models.nn.sequential.callbacks import (\n",
    "    ValidationMetricsCallback,\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback,\n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback,\n",
    ")\n",
    "from replay.models.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.data.nn import SequenceTokenizer, SequentialDataset, TensorFeatureSource, TensorSchema, TensorFeatureInfo\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionDataset,\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecModel,\n",
    ")\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "VAL_DIR = 'val'\n",
    "\n",
    "\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'clickstream.pq'))\n",
    "df_eval = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')).join(df_clickstream, on='cookie', how='semi')\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "\n",
    "df_train = df_clickstream\n",
    "eval_users = df_eval['cookie'].unique().to_list()\n",
    "\n",
    "n_nodes = 30_000\n",
    "train_small = df_train.join(df_train.unique(subset=['node', 'cookie']).select('node').group_by('node').len().sort('len').tail(n_nodes).drop('len'),\n",
    "                        on='node')\n",
    "df_eval_small = df_eval.join(df_train, on='cookie', how='semi')\n",
    "df_eval_small = df_eval_small.with_columns(pl.col('node').cast(pl.Int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02df29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 51\n",
      "After: 51\n",
      "Items save 100.0 %\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    all_features = base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def filter_n_count(df, column='item_id', n=5):\n",
    "    to_save = df.group_by(column).agg(pl.count().alias('count')).filter(pl.col('count')>n)\n",
    "    return df.join(to_save, on=column, how='semi')\n",
    "\n",
    "def allign_gt(df, gt, item_col='item_id', user_col='user_id'):\n",
    "    gt = gt.join(df, on=user_col, how='semi')\n",
    "    gt = gt.join(df, on=item_col, how='semi')\n",
    "    return gt\n",
    "\n",
    "def make_replay_format(df, is_gt=False):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col('cookie').alias('user_id').cast(pl.Int64), \n",
    "            pl.col('category').alias('item_id').cast(pl.Int64)\n",
    "        ])\n",
    "    if not is_gt:\n",
    "        df = df.with_columns(pl.col(\"event_date\").dt.timestamp(\"ms\").alias(\"timestamp\") // 1000)\n",
    "    return df\n",
    "\n",
    "validation_gt = make_replay_format(df_eval_small.join(df_cat_features.with_columns(pl.col('node').cast(pl.Int64)).drop_nulls().select('node', 'category').unique(), on='node', how='left'), is_gt=True)['user_id', 'item_id'].unique()\n",
    "train_events = make_replay_format(train_small.join(df_cat_features.drop_nulls().select('node', 'category').unique(), on='node', how='left'))['user_id', 'item_id','timestamp']\n",
    "item_before = train_events['item_id'].unique().len()\n",
    "item_after = train_events['item_id'].unique().len()\n",
    "\n",
    "print(f'Before: {item_before}')\n",
    "print(f'After: {item_after}')\n",
    "print(f'Items save {100 * item_after/item_before} %')\n",
    "validation_gt = allign_gt(train_events, validation_gt)\n",
    "item_features = train_events[['item_id']].unique()\n",
    "validation_events = train_events\n",
    "\n",
    "user_features = make_replay_format(df_clickstream.join(df_cat_features.drop_nulls().select('node', 'category').unique(), on='node', how='left'))[['user_id']].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fe870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\3415952068.py:79: DeprecationWarning: SasRecTrainingDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecTrainingDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:79: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\3415952068.py:90: DeprecationWarning: SasRecValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:206: DeprecationWarning: TorchSequentialValidationDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialValidationDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\data\\nn\\torch_sequential_dataset.py:235: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\code\\avito_hack_clear\\.checkpoints\\sasrec_category exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type             | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | _model | SasRecModel      | 218 K  | train\n",
      "1 | _loss  | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "218 K     Trainable params\n",
      "0         Non-trainable params\n",
      "218 K     Total params\n",
      "0.876     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c4825dff464232ab2796be434d7c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f49b73a841438cb13c2e16a2ea272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3d6d793d9c4d608f2cc5a135312690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 248: 'ndcg@5' reached 0.46160 (best 0.46160), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_category\\\\epoch=0-step=248-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.298366  0.411104  0.381566\n",
      "ndcg    0.298366  0.518872  0.461601\n",
      "recall  0.212957  0.765572  0.609367\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0336f42d9344dc2a9096e7603bd1231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 496: 'ndcg@5' reached 0.46626 (best 0.46626), saving model to 'C:\\\\code\\\\avito_hack_clear\\\\.checkpoints\\\\sasrec_category\\\\epoch=1-step=496.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.304509  0.415716  0.386477\n",
      "ndcg    0.304509  0.522943  0.466262\n",
      "recall  0.217589  0.767836  0.613223\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d2461321cd44009600a4bf4fdadf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 744: 'ndcg@5' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.304745  0.415012  0.385685\n",
      "ndcg    0.304745  0.522484  0.465802\n",
      "recall  0.217629  0.768077  0.613735\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cd275f31864dff85ba725d2b079bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 992: 'ndcg@5' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.301401  0.412559  0.382953\n",
      "ndcg    0.301401  0.520390  0.463110\n",
      "recall  0.214911  0.767287  0.611289\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf98c7477064cfa926f647890e4aeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1240: 'ndcg@5' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10         5\n",
      "map     0.298312  0.410138  0.380102\n",
      "ndcg    0.298312  0.518359  0.460306\n",
      "recall  0.212801  0.766803  0.608767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 9\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=ITEM_FEATURE_NAME,\n",
    "            is_seq=True,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column)],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "sequential_train_dataset = tokenizer.transform(train_dataset)\n",
    "\n",
    "sequential_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "sequential_validation_gt = tokenizer.transform(validation_gt, [tensor_schema.item_id_feature_name])\n",
    "\n",
    "sequential_validation_dataset, sequential_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "    sequential_validation_dataset, sequential_validation_gt\n",
    ")\n",
    "\n",
    "model = SasRec(\n",
    "    tensor_schema,\n",
    "    block_count=2,\n",
    "    head_count=2,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_size=128,\n",
    "    dropout_rate=0.3,\n",
    "    optimizer_factory=FatOptimizerFactory(learning_rate=0.001),\n",
    "    loss_sample_count=50,\n",
    "    negatives_sharing=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SASRec_category\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints/sasrec_category\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"ndcg@5\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=SasRecValidationDataset(\n",
    "        sequential_validation_dataset,\n",
    "        sequential_validation_gt,\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"recall\", 'map', 'ndcg'],\n",
    "    ks=[1, 5, 10],\n",
    "    item_count=train_dataset.item_count,\n",
    "    postprocessors=[],\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    "    accelerator='cuda',\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=validation_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dfe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_628\\2460397807.py:7: DeprecationWarning: SasRecPredictionDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  dataset=SasRecPredictionDataset(\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\replay\\models\\nn\\sequential\\sasrec\\dataset.py:146: DeprecationWarning: TorchSequentialDataset.__init__ `padding_value` parameter will be removed in future versions. Instead, you should specify `padding_value` for each column in TensorSchema\n",
      "  self._inner = TorchSequentialDataset(\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Grig\\miniconda3\\envs\\my_env_py3.9\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b5c233c739416881f30a21598c7941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "PREDICTION_PATH = 'predictions'\n",
    "MODEL_NAME = 'SASREC_REPLAY_CATEGORY'\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=SasRecPredictionDataset(\n",
    "        sequential_validation_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SASRec_category\")\n",
    "\n",
    "TOPK = [50]\n",
    "\n",
    "postprocessors = []\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        pandas_prediction_callback,\n",
    "    ],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True,\n",
    ")\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(pandas_res)\n",
    "recommendations = pl.from_pandas(recommendations)\n",
    "\n",
    "os.makedirs(os.path.join(PREDICTION_PATH, MODEL_NAME), exist_ok=True)\n",
    "recommendations.rename({'user_id':'cookie', 'item_id':'category'}).write_parquet(os.path.join(PREDICTION_PATH, MODEL_NAME, 'SASREC_CATEGORY_REPLAY_VAL.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5fabd",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from utils import cast_dtypes, get_dataset\n",
    "DATA_DIR = 'data'\n",
    "VAL_DIR = 'val'\n",
    "TRAIN_DIR = 'train'\n",
    "df_clickstream = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'clickstream.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4a1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pl.read_parquet(os.path.join(DATA_DIR, 'events.pq'))\n",
    "df_cat_features = pl.read_parquet(os.path.join(DATA_DIR, 'cat_features.pq'))\n",
    "\n",
    "def make_user_user_category(df_clickstream):\n",
    "    user_features = (df_clickstream\n",
    "    .join(df_event, on='event')\n",
    "    .select('cookie', 'is_contact').with_columns(value=pl.lit(1)).pivot(\n",
    "        values=\"value\",\n",
    "        index=\"cookie\",\n",
    "        columns=\"is_contact\",\n",
    "        aggregate_function=\"sum\",\n",
    "    ).fill_null(0)).with_columns(pl.col('0').alias('is_contact_0'),\n",
    "                                pl.col('1').alias('is_contact_1'),\n",
    "                                contact_ratio = (pl.col('1') / pl.col('0'))\n",
    "                                ).drop('0', '1').with_columns(\n",
    "                                pl.when(pl.col(\"contact_ratio\").is_infinite())\n",
    "                                .then(0)\n",
    "                                .otherwise(pl.col(\"contact_ratio\"))\n",
    "                                .alias(\"contact_ratio\")\n",
    "                            )\n",
    "\n",
    "    user_last_category_contact = (df_clickstream\n",
    "                                .join(df_event, on='event')\n",
    "                                .join(df_cat_features.select('item', 'category'), on='item')\n",
    "                                .filter(pl.col('is_contact')>0)\n",
    "                                .sort('cookie', 'event_date')\n",
    "                                .unique(subset=['cookie'], keep='last')\n",
    "                                .select('cookie', pl.col('category').alias('last_contact_category'))\n",
    "    )\n",
    "\n",
    "    user_last_category_seen = (df_clickstream\n",
    "                                .join(df_event, on='event')\n",
    "                                .join(df_cat_features.select('item', 'category'), on='item')\n",
    "                                .sort('cookie', 'event_date')\n",
    "                                .unique(subset=['cookie'], keep='last')\n",
    "                                .select('cookie', pl.col('category').alias('last_seen_category'))\n",
    "    )\n",
    "    \n",
    "    user_features = user_features.join(user_last_category_contact, on='cookie', how='left').join(user_last_category_seen, on='cookie', how='left')\n",
    "\n",
    "    user_category_total = (df_clickstream\n",
    "                        .join(df_event, on='event')\n",
    "                        .join(df_cat_features.select('item', 'category'), on='item', how='left')\n",
    "                        .group_by('cookie', 'category')\n",
    "                        .agg(pl.len().alias('user_total_category_count'))\n",
    "    )\n",
    "\n",
    "    user_category_pos = (df_clickstream\n",
    "                        .join(df_event, on='event')\n",
    "                        .filter(pl.col('is_contact')==1)\n",
    "                        .join(df_cat_features.select('item', 'category'), on='item', how='left')\n",
    "                        .group_by('cookie', 'category')\n",
    "                        .agg(pl.len().alias('user_pos_category_count'))\n",
    "    )\n",
    "\n",
    "    user_category = user_category_total.join(user_category_pos, on=['cookie', 'category'], how='left').fill_null(0)\n",
    "    user_category = user_category.join(df_clickstream.join(df_event, on='event', how='left').group_by('cookie').agg(pl.len().alias('user_total_rows'), pl.sum('is_contact').alias('total_contacts')), on='cookie', how='left')\n",
    "    user_category = user_category.with_columns(\n",
    "        (pl.col('user_pos_category_count') / pl.col('user_total_category_count')).alias('user_category_contact_ratio'),\n",
    "        (pl.col('user_pos_category_count') / pl.col('total_contacts')).alias('ratio_of_this_category_in_all_user_contact'),\n",
    "        (pl.col('user_total_category_count') / pl.col('user_total_rows')).alias('ratio_user_category_click'))\n",
    "\n",
    "    return user_features, user_category\n",
    "\n",
    "def make_item_features(df_clickstream):\n",
    "    df_clickstream = df_clickstream.drop_nulls()\n",
    "    item_features = (df_clickstream\n",
    "    .join(df_event, on='event')\n",
    "    .select('node', 'is_contact')\n",
    "    .with_columns(value=pl.lit(1))\n",
    "    .pivot(\n",
    "        values=\"value\",\n",
    "        index=\"node\",\n",
    "        columns=\"is_contact\",\n",
    "        aggregate_function=\"sum\",\n",
    "    ).fill_null(0)\n",
    "    .with_columns(pl.col('0').alias('node_contact_0'),\n",
    "                pl.col('1').alias('node_contact_1'),\n",
    "                contact_ratio = (pl.col('1') / pl.col('0'))\n",
    "                )\n",
    "    .drop('0', '1')\n",
    "    .with_columns(\n",
    "                pl.when(pl.col(\"contact_ratio\").is_infinite())\n",
    "                .then(0)\n",
    "                .otherwise(pl.col(\"contact_ratio\"))\n",
    "                .alias(\"node_contact_ratio\")\n",
    "            )\n",
    "    .drop('contact_ratio')\n",
    "    )\n",
    "\n",
    "    item_features_unique = (df_clickstream\n",
    "    .join(df_event, on='event')\n",
    "    .select('cookie', 'node', 'is_contact', 'event_date').sort('event_date')\n",
    "    .unique(subset=['cookie', 'node', 'is_contact'], keep='last')\n",
    "    .select('node', 'is_contact').with_columns(value=pl.lit(1))\n",
    "    .pivot(\n",
    "        values=\"value\",\n",
    "        index=\"node\",\n",
    "        columns=\"is_contact\",\n",
    "        aggregate_function=\"sum\",\n",
    "    ).fill_null(0)).with_columns(pl.col('0').alias('node_contacts_0_last'),\n",
    "                                pl.col('1').alias('node_contacts_1_last'),\n",
    "                                contact_ratio = (pl.col('1') / pl.col('0'))\n",
    "                                ).drop('0', '1').with_columns(\n",
    "                                pl.when(pl.col(\"contact_ratio\").is_infinite())\n",
    "                                .then(0)\n",
    "                                .otherwise(pl.col(\"contact_ratio\"))\n",
    "                                .alias(\"node_contact_ratio_last\")\n",
    "                            ).drop('contact_ratio')\n",
    "\n",
    "    item_features_category = (df_clickstream\n",
    "    .join(df_event, on='event')\n",
    "    .join(df_cat_features.select('item', 'category').unique(), on='item', how='left')\n",
    "    .drop_nulls()\n",
    "    .select('category', 'is_contact').with_columns(value=pl.lit(1)).pivot(\n",
    "        values=\"value\",\n",
    "        index=\"category\",\n",
    "        columns=\"is_contact\",\n",
    "        aggregate_function=\"sum\",\n",
    "    ).fill_null(0)).with_columns(pl.col('0').alias('category_contact_0'),\n",
    "                                pl.col('1').alias('category_contact_1'),\n",
    "                                contact_ratio = (pl.col('1') / pl.col('0'))\n",
    "                                ).drop('0', '1').with_columns(\n",
    "                                pl.when(pl.col(\"contact_ratio\").is_infinite())\n",
    "                                .then(0)\n",
    "                                .otherwise(pl.col(\"contact_ratio\"))\n",
    "                                .alias(\"category_contact_ratio\")\n",
    "                            ).drop('contact_ratio')\n",
    "    \n",
    "    item_features_category_unique = (df_clickstream\n",
    "    .join(df_event, on='event')\n",
    "    .join(df_cat_features.select('item', 'category').unique(), on='item', how='left')\n",
    "    .unique(subset=['cookie', 'category', 'is_contact'], keep='last')\n",
    "    .drop_nulls()\n",
    "    .select('category', 'is_contact').with_columns(value=pl.lit(1)).pivot(\n",
    "        values=\"value\",\n",
    "        index=\"category\",\n",
    "        columns=\"is_contact\",\n",
    "        aggregate_function=\"sum\",\n",
    "    ).fill_null(0)).with_columns(pl.col('0').alias('category_contact_0_last'),\n",
    "                                pl.col('1').alias('category_contact_1_last'),\n",
    "                                contact_ratio = (pl.col('1') / pl.col('0'))\n",
    "                                ).drop('0', '1').with_columns(\n",
    "                                pl.when(pl.col(\"contact_ratio\").is_infinite())\n",
    "                                .then(0)\n",
    "                                .otherwise(pl.col(\"contact_ratio\"))\n",
    "                                .alias(\"category_contact_ratio_last\")\n",
    "                            ).drop('contact_ratio')\n",
    "\n",
    "    item_features = item_features.join(item_features_unique, on='node', how='left')\n",
    "    item_features = item_features.join(df_cat_features.drop_nulls().select('node', 'category').unique(), on='node').join(item_features_category, on='category', how='left').join(item_features_category_unique, on='category', how='left')\n",
    "\n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbf6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_location = (\n",
    "        df_clickstream.drop('platform', 'surface')\n",
    "        .join(df_cat_features.select('item','location', 'category'), how='left', on='item')\n",
    "        .group_by(['cookie', 'location'])\n",
    "        .agg(pl.len().alias('len'))\n",
    "        .sort('len', descending=True)\n",
    "        .unique(subset=['cookie'], keep='first')\n",
    "        .sort('cookie')\n",
    "        .drop('len')\n",
    ")\n",
    "\n",
    "item_location = df_cat_features.group_by('node', 'location').agg(pl.len().alias('location_in_node_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b02af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_836\\1543143575.py:5: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  user_features = (df_clickstream\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_836\\1543143575.py:67: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  item_features = (df_clickstream\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_836\\1543143575.py:91: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  item_features_unique = (df_clickstream\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_836\\1543143575.py:111: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  item_features_category = (df_clickstream\n",
      "C:\\Users\\Grig\\AppData\\Local\\Temp\\ipykernel_836\\1543143575.py:130: DeprecationWarning: The argument `columns` for `DataFrame.pivot` is deprecated. It has been renamed to `on`.\n",
      "  item_features_category_unique = (df_clickstream\n"
     ]
    }
   ],
   "source": [
    "user_features, user_category = make_user_user_category(df_clickstream)\n",
    "item_features = make_item_features(df_clickstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec_scores = (pl.read_parquet(os.path.join('predictions', 'SASREC_REPLAY_NODE', 'SASREC_NODE_REPLAY_VAL.pq'))\n",
    "                .with_columns( pl.col('score').rank(descending=True).over('cookie').alias('sasrec_rank'), \n",
    "                               pl.col('score').alias('sasrec_score')).drop('score')\n",
    ")\n",
    "\n",
    "sasrec_scores_category = (pl.read_parquet(os.path.join('predictions', 'SASREC_REPLAY_CATEGORY', 'SASREC_CATEGORY_REPLAY_VAL.pq'))\n",
    "                        .with_columns(pl.col('score').rank(descending=True).over('cookie').alias('sasrec_category_rank'), \n",
    "                                        pl.col('score').alias('sasrec_category_score'))\n",
    "                        .drop('score')\n",
    ")\n",
    "\n",
    "ease_preds = (pl\n",
    "              .read_parquet(os.path.join('predictions', 'EASE_DAN', 'EASE_DAN_val.pq'))\n",
    "              .select(pl.col('rank_rd').alias('ease_rank'), pl.col('score').alias('ease_score'), 'node', 'cookie')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf8fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pl.read_parquet(os.path.join('predictions', 'RDLAE', 'RDLAE_VAL.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f56070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = cast_dtypes(recs.with_columns(pl.col('score').rank(descending=True).over('cookie').alias('rank_rd')))\n",
    "tmp = tmp.join(cast_dtypes(most_pop_location), how='left', on=['cookie'])\n",
    "tmp = tmp.join(cast_dtypes(sasrec_scores), on=['cookie', 'node'], how='left')\n",
    "tmp = tmp.join(cast_dtypes(ease_preds), on=['cookie', 'node'], how='left')\n",
    "tmp = tmp.join(cast_dtypes(item_location), how='left', on=['node', 'location'])\n",
    "tmp = tmp.join(cast_dtypes(user_features), how='left', on=['cookie'])\n",
    "tmp = tmp.join(cast_dtypes(item_features), how='left', on=['node'])\n",
    "tmp = tmp.join(cast_dtypes(user_category), how='left', on=['cookie', 'category'])\n",
    "tmp = tmp.join(cast_dtypes(sasrec_scores_category), on=['cookie', 'category'], how='left')\n",
    "tmp = tmp.fill_null(0).with_columns(\n",
    "    (pl.col('category') == pl.col('last_contact_category')).alias('category_same_as_last_contact').cast(pl.Int16), \n",
    "    (pl.col('category') == pl.col('last_seen_category')).alias('category_same_as_last_seen').cast(pl.Int16),\n",
    "    (pl.col('node_contact_ratio') * pl.col('contact_ratio')).alias('user_node_ratio_prob'),\n",
    "    (pl.col('category_contact_ratio') * pl.col('contact_ratio')).alias('user_category_ratio_prob')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcdcc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(DATA_DIR, TRAIN_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5982d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.write_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'full_train.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "452a36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq'))\n",
    "sampled_train = get_dataset(tmp, df_eval=cast_dtypes(gt))\n",
    "sampled_train.write_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'sampled_train.pq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_inference = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, 'clickstream.pq')).group_by('node').len().sort('len').tail(30_000))\n",
    "\n",
    "node_in_inference = cast_dtypes(sampled_train.join(nodes_to_inference, how='semi', on='node'))\n",
    "node_missed_inference = cast_dtypes(sampled_train.join(nodes_to_inference, how='anti', on='node').with_columns(pl.lit(None).alias('node').cast(pl.UInt32))) # to not train on them\n",
    "\n",
    "sampled_train_masked = pl.concat([node_in_inference, node_missed_inference])\n",
    "sampled_train_masked.write_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'sampled_train_node_masked.pq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c90caa",
   "metadata": {},
   "source": [
    "# Ranker training LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d3b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from optuna import Trial, create_study\n",
    "from optuna.samplers import TPESampler\n",
    "from numpy import array, nan, random as np_rnd, where\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import recall_at, cast_dtypes\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_DIR = 'train'\n",
    "VAL_DIR = 'val'\n",
    "OPTUNA_LOGS_DIR_LGBM = '.logs/ranker/train/LGBM/'\n",
    "N_TRIALS_LGBM = 2 # to check that works\n",
    "train = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'sampled_train_node_masked.pq')))\n",
    "all_train = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'full_train.pq')))\n",
    "df_eval_val = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7d65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:04:30,105] A new study created in memory with name: no-name-a9ea66b3-f96f-491d-b3ce-ca6c0eca0f7a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a09bf3f34c0475b85a7f4b81dcd78aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:04:32,343] Trial 0 finished with value: 0.2004371654123786 and parameters: {'num_iterations': 210, 'application': 'rank_xendcg', 'learning_rate': 0.11186156011495693, 'num_leaves': 52, 'min_split_gain': 0.29999568707117197, 'lambda_l1': 0.11018816235014328, 'lambda_l2': 0.06024827634852292, 'feature_fraction': 0.34711443076608917, 'bagging_fraction': 0.21597659208548614, 'bagging_freq': 5, 'min_child_samples': 90}. Best is trial 0 with value: 0.2004371654123786.\n",
      "[I 2025-06-07 19:04:41,587] Trial 1 finished with value: 0.20858958071244202 and parameters: {'num_iterations': 413, 'application': 'rank_xendcg', 'learning_rate': 0.021538490945681114, 'num_leaves': 71, 'min_split_gain': 0.3518466511552321, 'lambda_l1': 0.4655623474987288, 'lambda_l2': 0.9036857606013757, 'feature_fraction': 0.6855027380913005, 'bagging_fraction': 0.3895883496313851, 'bagging_freq': 1, 'min_child_samples': 79}. Best is trial 1 with value: 0.20858958071244202.\n",
      "------------------------------\n",
      "№0:\n",
      "0.20858958071244202\n",
      "Лучшие гиперпараметры: {'num_iterations': 413, 'application': 'rank_xendcg', 'learning_rate': 0.021538490945681114, 'num_leaves': 71, 'min_split_gain': 0.3518466511552321, 'lambda_l1': 0.4655623474987288, 'lambda_l2': 0.9036857606013757, 'feature_fraction': 0.6855027380913005, 'bagging_fraction': 0.3895883496313851, 'bagging_freq': 1, 'min_child_samples': 79}\n",
      "------------------------------\n",
      "Training fold #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:04:41,997] A new study created in memory with name: no-name-3cab56d0-2a83-4b8a-be21-8c38b2c68f3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d99874ecde49ffb9b70b2ed92a9c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:04:44,051] Trial 0 finished with value: 0.20225114405704414 and parameters: {'num_iterations': 147, 'application': 'rank_xendcg', 'learning_rate': 0.0044055447260074035, 'num_leaves': 96, 'min_split_gain': 0.19166045702191514, 'lambda_l1': 0.2814166614878487, 'lambda_l2': 0.18383393819202742, 'feature_fraction': 0.10576555562503079, 'bagging_fraction': 0.3344178341022963, 'bagging_freq': 1, 'min_child_samples': 14}. Best is trial 0 with value: 0.20225114405704414.\n",
      "[I 2025-06-07 19:04:49,724] Trial 1 finished with value: 0.14678961505731422 and parameters: {'num_iterations': 99, 'application': 'lambdarank', 'learning_rate': 0.2230683198746363, 'num_leaves': 93, 'min_split_gain': 0.21806007212326106, 'lambda_l1': 0.07197371138873832, 'lambda_l2': 0.21169013939409442, 'feature_fraction': 0.6652381110229788, 'bagging_fraction': 0.38169736916112806, 'bagging_freq': 3, 'min_child_samples': 72}. Best is trial 0 with value: 0.20225114405704414.\n",
      "------------------------------\n",
      "№1:\n",
      "0.20225114405704414\n",
      "Лучшие гиперпараметры: {'num_iterations': 147, 'application': 'rank_xendcg', 'learning_rate': 0.0044055447260074035, 'num_leaves': 96, 'min_split_gain': 0.19166045702191514, 'lambda_l1': 0.2814166614878487, 'lambda_l2': 0.18383393819202742, 'feature_fraction': 0.10576555562503079, 'bagging_fraction': 0.3344178341022963, 'bagging_freq': 1, 'min_child_samples': 14}\n",
      "------------------------------\n",
      "Training fold #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:04:50,112] A new study created in memory with name: no-name-9e457c8d-fcd5-478c-8595-b5a4e3166592\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4225ae752d4ca8af84825c24536203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:05:06,477] Trial 0 finished with value: 0.1873818111726522 and parameters: {'num_iterations': 288, 'application': 'lambdarank', 'learning_rate': 0.0018824990109750559, 'num_leaves': 62, 'min_split_gain': 0.38723196529165654, 'lambda_l1': 0.11784375551822668, 'lambda_l2': 0.9185241226449185, 'feature_fraction': 0.7467860357828763, 'bagging_fraction': 0.22086406038186657, 'bagging_freq': 3, 'min_child_samples': 14}. Best is trial 0 with value: 0.1873818111726522.\n",
      "[I 2025-06-07 19:05:15,974] Trial 1 finished with value: 0.1926743347860575 and parameters: {'num_iterations': 240, 'application': 'rank_xendcg', 'learning_rate': 0.025360911427982274, 'num_leaves': 73, 'min_split_gain': 0.3049980506032946, 'lambda_l1': 0.3407588424609349, 'lambda_l2': 0.6508531281979258, 'feature_fraction': 0.8969925338164916, 'bagging_fraction': 0.4932323252127493, 'bagging_freq': 2, 'min_child_samples': 50}. Best is trial 1 with value: 0.1926743347860575.\n",
      "------------------------------\n",
      "№2:\n",
      "0.1926743347860575\n",
      "Лучшие гиперпараметры: {'num_iterations': 240, 'application': 'rank_xendcg', 'learning_rate': 0.025360911427982274, 'num_leaves': 73, 'min_split_gain': 0.3049980506032946, 'lambda_l1': 0.3407588424609349, 'lambda_l2': 0.6508531281979258, 'feature_fraction': 0.8969925338164916, 'bagging_fraction': 0.4932323252127493, 'bagging_freq': 2, 'min_child_samples': 50}\n",
      "------------------------------\n",
      "Training fold #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:05:16,359] A new study created in memory with name: no-name-c4fba820-8874-4a55-a4f5-3e004b0447dd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa1f86ac478446496ddaf220a96a516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:05:43,377] Trial 0 finished with value: 0.17278459465378476 and parameters: {'num_iterations': 306, 'application': 'lambdarank', 'learning_rate': 0.06615285206771772, 'num_leaves': 98, 'min_split_gain': 0.17637240531072557, 'lambda_l1': 0.32842891870287716, 'lambda_l2': 0.5803049688087442, 'feature_fraction': 0.862244336967427, 'bagging_fraction': 0.6505528426960532, 'bagging_freq': 5, 'min_child_samples': 64}. Best is trial 0 with value: 0.17278459465378476.\n",
      "[I 2025-06-07 19:05:44,470] Trial 1 finished with value: 0.20425156971108274 and parameters: {'num_iterations': 21, 'application': 'rank_xendcg', 'learning_rate': 0.006553331792581535, 'num_leaves': 29, 'min_split_gain': 0.20508677111090412, 'lambda_l1': 0.08841801128528293, 'lambda_l2': 0.36505672318774574, 'feature_fraction': 0.6407012136387936, 'bagging_fraction': 0.6391607308461834, 'bagging_freq': 5, 'min_child_samples': 53}. Best is trial 1 with value: 0.20425156971108274.\n",
      "------------------------------\n",
      "№3:\n",
      "0.20425156971108274\n",
      "Лучшие гиперпараметры: {'num_iterations': 21, 'application': 'rank_xendcg', 'learning_rate': 0.006553331792581535, 'num_leaves': 29, 'min_split_gain': 0.20508677111090412, 'lambda_l1': 0.08841801128528293, 'lambda_l2': 0.36505672318774574, 'feature_fraction': 0.6407012136387936, 'bagging_fraction': 0.6391607308461834, 'bagging_freq': 5, 'min_child_samples': 53}\n",
      "------------------------------\n",
      "Training fold #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:05:44,873] A new study created in memory with name: no-name-62736798-7817-41ef-b4a4-b6f2927a0203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b876d19713e436b8f39929c25c38663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:05:55,456] Trial 0 finished with value: 0.19594622190336755 and parameters: {'num_iterations': 189, 'application': 'lambdarank', 'learning_rate': 0.009252792365499463, 'num_leaves': 41, 'min_split_gain': 0.0942549862027911, 'lambda_l1': 0.9716892736918116, 'lambda_l2': 0.2948104880257191, 'feature_fraction': 0.9843975843457238, 'bagging_fraction': 0.9609009839341035, 'bagging_freq': 4, 'min_child_samples': 19}. Best is trial 0 with value: 0.19594622190336755.\n",
      "[I 2025-06-07 19:06:00,860] Trial 1 finished with value: 0.19563747862502007 and parameters: {'num_iterations': 375, 'application': 'rank_xendcg', 'learning_rate': 0.1362847295749121, 'num_leaves': 40, 'min_split_gain': 0.403265439548652, 'lambda_l1': 0.6090509367204264, 'lambda_l2': 0.2800898935660521, 'feature_fraction': 0.6480540571026433, 'bagging_fraction': 0.6443148373688176, 'bagging_freq': 4, 'min_child_samples': 43}. Best is trial 0 with value: 0.19594622190336755.\n",
      "------------------------------\n",
      "№4:\n",
      "0.19594622190336755\n",
      "Лучшие гиперпараметры: {'num_iterations': 189, 'application': 'lambdarank', 'learning_rate': 0.009252792365499463, 'num_leaves': 41, 'min_split_gain': 0.0942549862027911, 'lambda_l1': 0.9716892736918116, 'lambda_l2': 0.2948104880257191, 'feature_fraction': 0.9843975843457238, 'bagging_fraction': 0.9609009839341035, 'bagging_freq': 4, 'min_child_samples': 19}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import polars as pl\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "groups = train['cookie']\n",
    "\n",
    "cat_features = ['location', 'category', 'node', 'last_contact_category', 'last_seen_category', 'category_same_as_last_contact', 'category_same_as_last_seen'] #'last_contact_category', 'last_seen_category', 'category_same_as_last_contact', 'category_same_as_last_seen']\n",
    "\n",
    "def objective(trial, cat_features, train_x, train_y, train_groups, valid):\n",
    "\n",
    "        params = {\n",
    "            'num_iterations': trial.suggest_int('num_iterations', 20, 500),\n",
    "            'application': trial.suggest_categorical('application', ['lambdarank', 'rank_xendcg']), \n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "            'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.5),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 0, 5),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'seed': 42,\n",
    "            'device': 'CPU',\n",
    "            'verbosity': -1,\n",
    "            'n_jobs': 10,\n",
    "        }\n",
    "        cat_features_use = set(cat_features)\n",
    "        train_pool = lgb.Dataset(data=train_x,\n",
    "             label=train_y, group=train_groups, categorical_feature=list(cat_features_use))\n",
    "\n",
    "        ranker = lgb.train(params, \n",
    "                           train_pool)\n",
    "        preds = ranker.predict(valid[train_x.columns],\n",
    "                               num_iteration=ranker.best_iteration)\n",
    "        valid['pred'] = preds\n",
    "\n",
    "        prediction = pl.DataFrame(valid[['score', 'node', 'cookie', 'pred']])\n",
    "        boost_recall = recall_at(df_eval_val['cookie', 'node'].unique().join(prediction, on='cookie', how='semi'), prediction.drop('score').rename({'pred':'score'}), k=40)\n",
    "    \n",
    "        return boost_recall\n",
    "\n",
    "for fold_num, (train_u, valid_u) in enumerate(kfold.split(df_eval_val.select('cookie').unique())):\n",
    "    train_u = pl.DataFrame({'cookie':train_u}).with_columns(pl.col('cookie').cast(pl.Int64))\n",
    "    valid_u = pl.DataFrame({'cookie':valid_u}).with_columns(pl.col('cookie').cast(pl.Int64))\n",
    "    print(f'Training fold #{fold_num + 1}')\n",
    "    \n",
    "    train_part = train.join(train_u, on='cookie', how='semi')\n",
    "    valid_part = all_train.filter(pl.col('rank_rd')<301).join(valid_u, on='cookie', how='semi')\n",
    "    \n",
    "    train_part, valid_part = train_part.sort(by=['cookie']), valid_part.sort(by=['cookie'])\n",
    "    train_data, train_label, train_group = train_part.drop(['cookie', 'target']).to_pandas(), train_part['target'].to_numpy().reshape(-1), train_part.group_by('cookie').len().sort('cookie')['len'].to_numpy()\n",
    "    valid_data = valid_part.to_pandas()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(lambda trial: objective(trial, cat_features, train_data, train_label, train_group, valid_data), n_trials=N_TRIALS_LGBM, show_progress_bar=True)\n",
    "    print('---'*10)\n",
    "    print(f'№{fold_num}:\\n{study.best_value}') \n",
    "    print(f\"Лучшие гиперпараметры: {study.best_params}\") \n",
    "    print('---'*10)\n",
    "    os.makedirs(OPTUNA_LOGS_DIR_LGBM, exist_ok=True)\n",
    "    with open(OPTUNA_LOGS_DIR_LGBM + 'logs.txt', 'a') as the_file:\n",
    "        the_file.write(f\"{fold_num}\\n{study.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c63739",
   "metadata": {},
   "source": [
    "# Ranker training catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a6fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from optuna import Trial, create_study\n",
    "from optuna.samplers import TPESampler\n",
    "from numpy import array, nan, random as np_rnd, where\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import recall_at, cast_dtypes\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_DIR = 'train'\n",
    "VAL_DIR = 'val'\n",
    "OPTUNA_LOGS_DIR_CBR = '.logs/ranker/train/CBR/'\n",
    "N_TRIALS_CBR = 2 # to check that works\n",
    "train = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'sampled_train_node_masked.pq')))\n",
    "all_train = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, TRAIN_DIR, 'full_train.pq')))\n",
    "df_eval_val = cast_dtypes(pl.read_parquet(os.path.join(DATA_DIR, VAL_DIR, 'gt.pq')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ed3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:12:16,515] A new study created in memory with name: no-name-aab86b70-5f90-43c6-9ab9-d846f99d7f9c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fb408ac2ce48a1b7b417fc8b365cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:12:50,974] Trial 0 finished with value: 0.21906565577917425 and parameters: {'iterations': 757, 'learning_rate': 0.24786018076230062, 'depth': 8, 'l2_leaf_reg': 4.1976262580382, 'bootstrap_type': 'No', 'random_strength': 0.7283477684968298}. Best is trial 0 with value: 0.21906565577917425.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:13:13,709] Trial 1 finished with value: 0.21868159140302393 and parameters: {'iterations': 677, 'learning_rate': 0.09234405126308844, 'depth': 6, 'l2_leaf_reg': 2.315685472913124, 'bootstrap_type': 'No', 'random_strength': 1.3335452305004367}. Best is trial 0 with value: 0.21906565577917425.\n",
      "------------------------------\n",
      "№0:\n",
      "0.21906565577917425\n",
      "Лучшие гиперпараметры: {'iterations': 757, 'learning_rate': 0.24786018076230062, 'depth': 8, 'l2_leaf_reg': 4.1976262580382, 'bootstrap_type': 'No', 'random_strength': 0.7283477684968298}\n",
      "------------------------------\n",
      "Training fold #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:13:14,178] A new study created in memory with name: no-name-07c455cc-93c2-410a-a970-6181c11be419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fe0375ad8648ca8f572a15d0cd3ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:13:32,591] Trial 0 finished with value: 0.2111534140255227 and parameters: {'iterations': 351, 'learning_rate': 0.2232014333731204, 'depth': 8, 'l2_leaf_reg': 2.7417234889530593, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.0638215123177908}. Best is trial 0 with value: 0.2111534140255227.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:13:37,321] Trial 1 finished with value: 0.20839897763009796 and parameters: {'iterations': 108, 'learning_rate': 0.21155779689598522, 'depth': 5, 'l2_leaf_reg': 4.6885112316828, 'bootstrap_type': 'No', 'random_strength': 0.9864033320056874}. Best is trial 0 with value: 0.2111534140255227.\n",
      "------------------------------\n",
      "№1:\n",
      "0.2111534140255227\n",
      "Лучшие гиперпараметры: {'iterations': 351, 'learning_rate': 0.2232014333731204, 'depth': 8, 'l2_leaf_reg': 2.7417234889530593, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.0638215123177908}\n",
      "------------------------------\n",
      "Training fold #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:13:37,755] A new study created in memory with name: no-name-97dca122-6f8e-4a7d-a2e7-0bdd8abe9337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4f8894e59348afa2f7ca7e2d333cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:14:17,417] Trial 0 finished with value: 0.20503230175029596 and parameters: {'iterations': 905, 'learning_rate': 0.21430922888861717, 'depth': 8, 'l2_leaf_reg': 3.5278359932756085, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.114136727526179}. Best is trial 0 with value: 0.20503230175029596.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:14:42,141] Trial 1 finished with value: 0.20260510317018177 and parameters: {'iterations': 780, 'learning_rate': 0.021594506698682375, 'depth': 5, 'l2_leaf_reg': 4.2429177244862455, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.7728296434363517}. Best is trial 0 with value: 0.20503230175029596.\n",
      "------------------------------\n",
      "№2:\n",
      "0.20503230175029596\n",
      "Лучшие гиперпараметры: {'iterations': 905, 'learning_rate': 0.21430922888861717, 'depth': 8, 'l2_leaf_reg': 3.5278359932756085, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.114136727526179}\n",
      "------------------------------\n",
      "Training fold #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:14:42,912] A new study created in memory with name: no-name-bf953cc5-688a-49be-adba-3747b3281030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4c9d93451741e2990d1c8593a90811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:14:55,259] Trial 0 finished with value: 0.20688562475577665 and parameters: {'iterations': 249, 'learning_rate': 0.0951981301681507, 'depth': 6, 'l2_leaf_reg': 3.7965209010349943, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.368337209693884}. Best is trial 0 with value: 0.20688562475577665.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:15:19,601] Trial 1 finished with value: 0.21481875112307902 and parameters: {'iterations': 500, 'learning_rate': 0.25724055582778155, 'depth': 8, 'l2_leaf_reg': 4.709921291950293, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.0393258235985976}. Best is trial 1 with value: 0.21481875112307902.\n",
      "------------------------------\n",
      "№3:\n",
      "0.21481875112307902\n",
      "Лучшие гиперпараметры: {'iterations': 500, 'learning_rate': 0.25724055582778155, 'depth': 8, 'l2_leaf_reg': 4.709921291950293, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.0393258235985976}\n",
      "------------------------------\n",
      "Training fold #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:15:20,009] A new study created in memory with name: no-name-6a023689-43e7-4413-a6e7-9f491be5788e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f826991d6864ef8b8508986e9e4cc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:16:13,535] Trial 0 finished with value: 0.2170823923088576 and parameters: {'iterations': 655, 'learning_rate': 0.2555301866365448, 'depth': 8, 'l2_leaf_reg': 3.6698045378125252, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.526537149330717}. Best is trial 0 with value: 0.2170823923088576.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because PFound is/are not implemented for GPU\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric PFound is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 19:16:45,780] Trial 1 finished with value: 0.21545684500453935 and parameters: {'iterations': 737, 'learning_rate': 0.05123550912801964, 'depth': 8, 'l2_leaf_reg': 4.229441398169865, 'bootstrap_type': 'No', 'random_strength': 0.6209315256543415}. Best is trial 0 with value: 0.2170823923088576.\n",
      "------------------------------\n",
      "№4:\n",
      "0.2170823923088576\n",
      "Лучшие гиперпараметры: {'iterations': 655, 'learning_rate': 0.2555301866365448, 'depth': 8, 'l2_leaf_reg': 3.6698045378125252, 'bootstrap_type': 'Bernoulli', 'random_strength': 1.526537149330717}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRanker, Pool, CatBoostError\n",
    "import optuna\n",
    "import polars as pl\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "groups = train['cookie']\n",
    "\n",
    "cat_features = ['location', 'category', 'node', 'last_contact_category', 'last_seen_category', 'category_same_as_last_contact', 'category_same_as_last_seen'] #'last_contact_category', 'last_seen_category', 'category_same_as_last_contact', 'category_same_as_last_seen']\n",
    "\n",
    "def objective(trial, cat_features, train_x, train_y, train_groups, valid):\n",
    "\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3), \n",
    "            'depth': trial.suggest_int('depth', 4, 8),                 \n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 5), \n",
    "            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bernoulli', 'No']),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0.5, 2.0),\n",
    "            'loss_function': 'YetiRank',  \n",
    "            'task_type': 'GPU',                               \n",
    "            'devices': '0',                                              \n",
    "            'verbose': False                                      \n",
    "        }\n",
    "        valid_x = valid.copy()\n",
    "        cat_features_use = list(set(cat_features))\n",
    "        train_pool = Pool(data=train_x, label=train_y, group_id=train_groups, cat_features=cat_features_use)\n",
    "        ranker = CatBoostRanker(**params)\n",
    "        ranker.fit(train_pool)\n",
    "        preds = ranker.predict(valid_x[train_x.columns])\n",
    "        valid_x['pred'] = preds\n",
    "\n",
    "        prediction = pl.DataFrame(valid_x[['score', 'node', 'cookie', 'pred']])\n",
    "        boost_recall = recall_at(df_eval_val['cookie', 'node'].unique().join(prediction, on='cookie', how='semi'), prediction.drop('score').rename({'pred':'score'}), k=40)\n",
    "       \n",
    "        return boost_recall\n",
    "\n",
    "for fold_num, (train_u, valid_u) in enumerate(kfold.split(df_eval_val.select('cookie').unique())):\n",
    "    train_u = pl.DataFrame({'cookie':train_u}).with_columns(pl.col('cookie').cast(pl.Int64))\n",
    "    valid_u = pl.DataFrame({'cookie':valid_u}).with_columns(pl.col('cookie').cast(pl.Int64))\n",
    "    print(f'Training fold #{fold_num + 1}')\n",
    "    \n",
    "    train_part = train.join(train_u, on='cookie', how='semi').fill_null(0)\n",
    "    valid_part = all_train.filter(pl.col('rank_rd')<301).join(valid_u, on='cookie', how='semi')\n",
    "    \n",
    "    train_part, valid_part = train_part.sort(by=['cookie']), valid_part.sort(by=['cookie'])\n",
    "    train_data, train_label, train_group = train_part.drop(['cookie', 'target']).to_pandas(), train_part['target'].to_numpy().reshape(-1), train_part['cookie'].to_numpy()\n",
    "    valid_data = valid_part.to_pandas()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(lambda trial: objective(trial, cat_features, train_data, train_label, train_group, valid_data), n_trials=N_TRIALS_CBR, show_progress_bar=True)\n",
    "    print('---'*10)\n",
    "    print(f'№{fold_num}:\\n{study.best_value}') \n",
    "    print(f\"Лучшие гиперпараметры: {study.best_params}\") \n",
    "    print('---'*10)\n",
    "    os.makedirs(OPTUNA_LOGS_DIR_CBR, exist_ok=True)\n",
    "    with open(OPTUNA_LOGS_DIR_CBR + 'logs.txt', 'a') as the_file:\n",
    "        the_file.write(f\"{fold_num}\\n{study.best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447adb1",
   "metadata": {},
   "source": [
    "# Для инференса все модели тренируются заново на полном кликстриме (SasRec, SasRec over category, RDLAE, EASE_DAN) (кроме бустингов, бустинги усредняются по фолдам, лучший сабмит был чисто на катбусте)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
